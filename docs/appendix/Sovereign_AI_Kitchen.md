# Private AI Kitchen 詳細解説
## description: Sovereign AI Kitchen（自給自足型AI基盤）スライドを1ページずつVitePress向けに詳細解説したガイド

# Private AI Kitchen（自給自足型AI基盤）詳細解説

## 目次

1. 第1章 表紙：Private AI Kitchen 構想
2. 第2章 なぜ今「AIの自炊」が必要か
3. 第3章 エレベーターピッチ：Private AI Kitchenとは何か
4. 第4章 コンセプト設計：ハイブリッド・ストレージ戦略
5. 第5章 スコープ定義：何を自前で持ち、何を利用するか
6. 第6章 導入ロードマップ：まずは「ご近所さん」を探す
7. 第7章 参照アーキテクチャ：エアギャップ環境の構成像
8. 第8章 価値の核心：動的マルチフレーバーAIサービング（Multi-LoRA）
9. 第9章 想定される「怖い話」と先回り対策
10. 第10章 4フェーズ導入ロードマップ
11. 第11章 現実的なトレードオフ設計
12. 第12章 導入要件整理（ハード/ソフト/人的スキル）
13. 第13章 まとめと次アクション
14. 付録A：図に出てくる重要用語集
15. 付録B：実務でよく使うコマンド集（OpenShift/LLM/運用）
16. 付録C：公開時のVitePress運用Tips

---

## 第1章 表紙：Private AI Kitchen 構想

![SAI1.NTTデータソブリンAIプロジェクト：インセプションデッキ（案）](/images/LLM/SAI1.NTTデータソブリンAIプロジェクト：インセプションデッキ（案）.jpg)

### このページの要点

この表紙は、単なるタイトルではなく、資料全体の主張を一言で表しています。

- **Private AI Kitchen** = 企業内でAIを「自炊」できる基盤
- 目指すものは、外部依存を減らしながらAI活用を継続可能にすること
- キーワードは **主権（Sovereignty）**, **閉域/エアギャップ**, **運用性**, **再現性**

### 詳細解説

「Kitchen（キッチン）」という比喩は実務的に非常に分かりやすいです。

- **食材** = ベースモデル / LoRA / データ
- **調理器具** = GPU / 推論サーバ / ストレージ / Kubernetes
- **レシピ** = YAML / デプロイ定義 / 推論パラメータ
- **衛生管理** = セキュリティ / 監査 / 証明書 / 権限管理
- **仕込み・配膳** = CI/CD / レジストリ / モデル配布 / Serving

つまり、AI活用を「1回動いたら終わり」のPoCではなく、**継続運用可能な生産活動**として捉えています。

### 用語解説

- **主権AI（Sovereign AI）**  
  データ・モデル・運用を自社または自組織の管理下に置き、外部サービス依存を最小化する考え方。
- **エアギャップ（Air Gap）**  
  インターネットなど外部ネットワークから物理/論理的に隔離された環境。
- **LLM（Large Language Model）**  
  大規模言語モデル。文章生成・要約・分類・会話などに利用。

### 実務Tips

- 表紙のタイトルは、公開時にそのままページタイトルに使うより、検索性を上げるために副題を付けるとよいです。  
  例：`Private AI Kitchen：閉域環境で実現する自給自足型AI基盤の設計と導入`

---

## 第2章 なぜ今「AIの自炊」が必要か

![SAI2.我々はなぜここにいるのか？](/images/LLM/SAI2.我々はなぜここにいるのか？.jpg)

### このページの要点

このページは、**問題提起（Why）** を担当しています。

対比構造になっており、概ね次の2つを比較しています。

- **クラウド依存型AI活用**
- **主権AI（自給自足型AI活用）**

### 図の読み方（左右比較）

#### 左側（クラウド依存側）で言いたいこと
- すぐ使えて便利
- ただし、長期的には
  - コスト増
  - データ持ち出し懸念
  - ベンダーロックイン
  - 規約変更・提供停止リスク
  - レイテンシ/接続制約
  が効いてくる

#### 右側（主権AI側）で言いたいこと
- 最初の構築コストは必要
- しかし一度整えば
  - データ統制
  - ポリシー統制
  - 内部事情に合わせた最適化
  - 中長期の安定運用
  が可能になる

### 詳細解説

この比較は、AI導入でよくある「使えるかどうか」の議論から一段進んで、**運用責任を誰が持つか** まで踏み込んでいます。

特にエンタープライズでは、AI導入の本当のボトルネックはモデル性能そのものではなく、以下です。

- データの扱い（機密/個人情報/契約情報）
- 監査対応（誰が何を入れたか）
- 継続予算（PoC後に止まる問題）
- 障害対応（ベンダー側障害時の業務影響）
- 組織横断の標準化（部署ごとにバラバラ問題）

このページは、その背景を「AIの自炊」という比喩で伝えています。

### 用語解説

- **ベンダーロックイン**  
  特定ベンダーの仕様やサービスに依存し、移行が困難になる状態。
- **レイテンシ**  
  応答の遅延時間。AI推論ではユーザー体感に直結。
- **データ統制**  
  データ保存場所・アクセス・持ち出し・削除などを組織ルールで制御すること。

### 実務Tips

- 社内説明では「クラウド vs オンプレ」の二元論にしないこと。  
  実際には **ハイブリッド**（用途ごとに使い分け）が現実解になりやすいです。
- まず「禁止事項」ではなく、「どのデータなら外部利用可/不可か」を定義すると議論が進みやすいです。

---

## 第3章 エレベーターピッチ：Private AI Kitchenとは何か

![SAI3.エレベータピッチ](/images/LLM/SAI3.エレベータピッチ.jpg)

### このページの要点

このページは、**経営層/非技術者向けの短い説明** を整理するページです。  
「何ができるのか」「何が嬉しいのか」を表形式で示します。

### 詳細解説

エレベーターピッチの役割は、技術詳細を省きつつ、判断に必要な要素だけを揃えることです。  
一般にこのページでは次の観点が並びます。

- **目的**：閉域/主権AI基盤の整備
- **提供価値**：安全性・俊敏性・再利用性
- **主な構成**：OpenShift AI / 推論サーバ / ストレージ / モデル管理
- **想定ユースケース**：社内RAG、要約、分類、業務支援
- **差別化要因**：LoRA活用、エアギャップ運用、複数モデル運用

### 読み手別の刺さるポイント

#### 技術者に刺さる点
- 再現性（YAML/コンテナ化）
- 監視/運用設計
- 拡張性（マルチモデル/マルチLoRA）

#### 管理者に刺さる点
- セキュリティ統制
- コスト見通し
- 部門横断の標準化

#### 現場利用者に刺さる点
- 使える速度
- 回答品質
- 業務適合性（日本語/社内用語）

### 用語解説

- **PoC（Proof of Concept）**  
  概念実証。小規模に実現可能性を確かめる段階。
- **RAG（Retrieval-Augmented Generation）**  
  検索した文書を参照して生成する仕組み。社内文書QAでよく使う。
- **YAML**  
  設定を記述するテキスト形式。Kubernetes/OpenShiftで多用される。

### 実務Tips

- このページは、会議の冒頭3分で話せる内容にすると強いです。  
  長い説明は後続ページに回し、ここでは「全体像」と「価値」だけに絞るのがコツです。

---

## 第4章 コンセプト設計：ハイブリッド・ストレージ戦略

![SAI4.パッケージデザイン](/images/LLM/SAI4.パッケージデザイン.jpg)

### このページの要点

このページは、**ベースモデルとLoRAアダプタを分けて管理する設計思想** を示しています。  
ここがこの資料の技術的な中核の1つです。

- ベースモデル（巨大・重い） → **PVC（Persistent Volume Claim）** に置く
- LoRAアダプタ（小さい・差し替え頻度高い） → **S3/MinIO（オブジェクトストレージ）** に置く

### 図の読み方（左→右）

1. **ベースモデルの格納先（PVC）**
   - 容量が大きい
   - 起動時/常駐で読むことが多い
   - 低レイテンシ・安定参照が重要

2. **LoRAアダプタの格納先（S3/MinIO）**
   - ファイルサイズが小さい
   - 個別用途ごとに増える
   - 動的ロード/切り替えに向く

3. **比較表**
   - サイズ・更新頻度・格納先・ロード方法などの観点で役割分担

### 詳細解説

この分離は、単なる「置き場所の工夫」ではなく、運用コストに直結します。

#### なぜ分けるのか？
- ベースモデルを毎回コピーすると遅い
- LoRAをベースと一体化すると配布が重くなる
- バージョン管理が煩雑になる
- ちょっとした用途追加でも全体デプロイになってしまう

#### 分離のメリット
- **配布時間短縮**（LoRAだけ配布）
- **保管効率**（重複を避ける）
- **切替容易性**（ユースケース別LoRA差し替え）
- **運用の責務分離**（基盤担当と業務AI担当を分けやすい）

### 用語解説

- **PVC (Persistent Volume Claim)**  
  Kubernetes/OpenShiftで永続ストレージを使うための要求定義。Podが再作成されてもデータを保持できる。
- **S3互換ストレージ**  
  AWS S3と同様のAPIで操作できるオブジェクトストレージ。MinIOは代表例。
- **LoRA（Low-Rank Adaptation）**  
  ベースモデル全体を再学習せず、一部パラメータ追加でタスク適応する軽量チューニング手法。
- **Adapter**  
  LoRAなどの追加学習済み差分ファイルの総称として使われることが多い。

### 実務Tips

- ベースモデルとLoRAは **命名規則を分ける** と事故が減ります。  
  例：
  - `base/tsuzumi-7b-instruct/v1/`
  - `lora/helpdesk-classifier/v2026-02-01/`
- LoRA配布時は、`adapter_config.json` と `adapter_model.safetensors` のセット管理を徹底すること。

---

## 第5章 スコープ定義：何を「自前」で持ち、何を「利用」するか

![SAI5.やらないことリスト](/images/LLM/SAI5.やらないことリスト.jpg)

### このページの要点

このページは、**「全部自前」ではなく、戦略的に境界を引く** ことを示しています。  
スコープの誤解を防ぐ、非常に重要なページです。

### 詳細解説

主権AIという言葉を聞くと、以下の誤解が起きがちです。

- GPUもソフトもモデルも全部フルスクラッチで作る？
- 外部技術は一切使わない？
- OSSも使えない？

このページの主張は、そうではなく、

> **自社/自組織の責任範囲を明確化し、コントロールすべきものを自前で持つ**

という整理です。

#### 典型的な「自前で持つ」対象
- データ
- モデル配布・承認プロセス
- 実行基盤（少なくとも制御面）
- ログ/監査
- 権限/認証ポリシー

#### 典型的な「利用する」対象
- OSSコンポーネント（vLLM, KServe, Istio, MinIO等）
- 商用基盤（OpenShift / OpenShift AI）
- ベースモデル（ライセンスと契約に従う）
- 学術手法（LoRA, PEFT等）

### 図の見方のポイント

- 「自前」と「利用」の境界が、ドーナツ図や円環で示されることが多い
- 中心に近いほど統制対象、外側ほど利用対象のイメージ
- 読むときは「責任分界点」を確認する

### 用語解説

- **責任分界点**  
  障害・運用・セキュリティ対応の責任がどこまで自組織にあるかを分ける境界。
- **OSS（Open Source Software）**  
  ソースコードが公開され、ライセンス条件のもとで利用可能なソフトウェア。
- **PEFT（Parameter-Efficient Fine-Tuning）**  
  少ない追加パラメータでモデル適応する手法群（LoRA含む）。

### 実務Tips

- スコープページは技術資料より先に合意しておくと、後で揉めにくいです。
- 「やる/やらない」だけでなく、**今期やる/次期やる** の段階分けを入れると現実的になります。

---

## 第6章 導入ロードマップ：まずは「ご近所さん」を探せ

![SAI6.ご近所さんを探せ](/images/LLM/SAI6.ご近所さんを探せ.jpg)

### このページの要点

このページは、技術導入の前に **関係者マップ（誰が何を持っているか）を把握せよ** という実務的メッセージです。

### 詳細解説

閉域・主権AI基盤の導入では、技術より先に **組織の壁** が立ちはだかります。  
このページの意図は、初手で「必要な人」を探すことです。

#### まず会いに行くべき関係者（例）
- **インフラ/基盤チーム**
  - OpenShiftクラスタ
  - GPUノード
  - ストレージ
  - ネットワーク/FW
- **セキュリティ/統制チーム**
  - 証明書
  - 監査ログ要件
  - ソフト持込/持出ルール
- **運用チーム**
  - 監視
  - 障害一次対応
  - バックアップ
- **業務部門/利用部門**
  - 何のためのAIか
  - 成功条件
  - NGデータの定義
- **法務/契約/購買**
  - モデルライセンス
  - ソフトウェア契約
  - サポート条件

### なぜこのページが重要か

AI基盤導入で失敗しやすいのは、技術不足よりも「前提未確認」です。

- GPUはあるが利用申請に1か月かかる
- MinIOはあるがバケット作成権限がない
- OpenShift AIは導入済みだがProject払い出し未完了
- 使いたいモデルのライセンス確認が未実施

このページはそれを防ぐための、いわば **着工前点検表** です。

### 用語解説

- **ステークホルダー**  
  プロジェクトに利害関係を持つ人・部門。
- **RACI**  
  誰が実行責任者（Responsible）か、承認者（Accountable）か等を整理する責任分担表。
- **運用受領**  
  開発側から運用側へ、運用可能な状態として引き渡すこと。

### 実務Tips

- 最初の1週間で「関係者・権限・環境」の棚卸しをすると、後工程の手戻りが大きく減ります。
- 「誰がバケットを作れるか」「誰がNamespaceを作れるか」は最初に確認。

---

## 第7章 参照アーキテクチャ：エアギャップ環境での Private AI Kitchen

![SAI7.解決策を描く：完全閉域アーキテクチャ](/images/LLM/SAI7.解決策を描く：完全閉域アーキテクチャ.jpg)

### このページの要点

このページは、資料全体の **実装イメージ図（リファレンス構成）** です。  
エアギャップ環境で、どの要素がどうつながるかを示しています。

### 図の主な構成要素（読み解き）

#### 1. 利用者/業務システム側
- 社内アプリやユーザーが推論APIを呼ぶ入口
- ここで認証・認可・入力制御が必要になる場合が多い

#### 2. OpenShift / OpenShift AI クラスタ
- **Namespace / Project** 単位でワークロード分離
- 推論コンポーネント（KServe / vLLM）を配置
- 監視（Prometheus/Grafana等）やログ収集も同居/連携

#### 3. 推論サーバ（vLLM等）
- ベースモデルを保持して推論実行
- LoRAアダプタを動的にロード（構成による）
- GPUを使って高速推論

#### 4. モデル/アダプタ保管（MinIO / Object Storage）
- ベースモデル or LoRAアダプタの格納先
- バージョニング/承認済みアーティファクト管理に使う

#### 5. 持込・配布経路（エアギャップ境界）
- 外部からのイメージ/モデル持込は厳格管理
- Bastion / 搬入端末 / 承認プロセスを経由
- チェックサム・署名・スキャンが重要

### 詳細解説（アーキテクチャの勘所）

#### 勘所1：データプレーンと管理プレーンを分けて考える
- **データプレーン**：推論処理そのもの（vLLM/KServe/GPU）
- **管理プレーン**：デプロイ、監視、承認、ログ、権限

この2つを分けて設計すると、障害切り分けがしやすくなります。

#### 勘所2：エアギャップでは「搬入プロセス」が機能要件になる
クラウド前提だと見落としがちですが、閉域環境では  
**どうやってコンテナイメージ・モデル・LoRAを持ち込むか** が最重要です。

#### 勘所3：OpenShift Project/Namespace分離
- 部門別
- ユースケース別
- 検証/本番別  
で分けると、権限やリソース管理が整理しやすいです。

### 用語解説

- **OpenShift**  
  Red HatのKubernetesベース基盤。企業向け運用機能が強い。
- **OpenShift AI (RHOAI)**  
  OpenShift上でAI/MLワークロードを扱うための機能群。
- **KServe**  
  Kubernetes上で推論サービスを標準的にデプロイ/運用するための仕組み。
- **vLLM**  
  LLM推論高速化のための推論エンジン。PagedAttention等で効率化。
- **Namespace / Project**  
  Kubernetes/OpenShiftにおける論理的な隔離単位（OpenShiftではProjectとして見せることが多い）。
- **Bastion（踏み台）**  
  制限環境へのアクセス中継や管理作業の拠点となるサーバ/端末。

### 実務Tips

- 参照アーキテクチャ図には、後から以下を追記すると実装が楽になります。
  - どこで認証するか
  - どこで監査ログを取るか
  - どこでモデル承認状態を判定するか
  - どこでロールバックするか

---

## 第8章 価値の核心：動的マルチフレーバーAIサービング（Multi-LoRA）

![SAI8.秘伝のたれ：Multi-LoRA戦略](/images/LLM/SAI8.秘伝のたれ：Multi-LoRA戦略.jpg)

### このページの要点

このページは、**Private AI Kitchenの技術的な目玉** を説明しています。

- 1つのベースモデルを常駐
- 用途別LoRAを差し替え/切り替え
- 「業務ごとのAI」を効率的に提供

### 「マルチフレーバー」の意味

ここでのフレーバーは、アイスの味のような比喩です。

- ベースは同じ（例：共通の日本語LLM）
- 用途ごとに味付け（LoRA）を変える
  - FAQ対応LoRA
  - 要約LoRA
  - 分類LoRA
  - ドメイン特化LoRA

これにより、**モデルを何本も丸ごと持つより軽く、速く、運用しやすい** ことが多いです。

### 詳細解説

#### 従来型（一体モデル複数本）との違い
- 一体モデル：用途ごとにベース+学習済みモデル全体を保持
- Multi-LoRA：ベースは共有、差分だけ切り替え

#### 効果
- GPUメモリ効率がよい（構成次第）
- ストレージ容量節約
- デプロイ/配布時間短縮
- 新用途追加が速い

#### 注意点
- LoRAごとの品質検証は別途必要
- ベースモデル更新時にLoRA再検証が必要になる場合あり
- 推論サーバ設定（max-loras, rank等）の調整が必要

### 用語解説

- **Multi-LoRA**  
  複数LoRAアダプタを同一推論基盤で扱う運用パターン（または機能）。
- **フレーバー**  
  ここでは「用途別/業務別のAI振る舞い（LoRA差し替え）」の比喩。
- **Rank（LoRAランク）**  
  LoRA差分の表現力に関わるパラメータ。大きいほど表現力↑、負荷/容量↑の傾向。

### 実務Tips

- まずは **2〜3個のLoRA** から始めるのがよいです（いきなり大量運用しない）。
- ベースモデル更新時は、LoRAごとに回帰テスト観点（回答品質/速度）を決めておくこと。

---

## 第9章 想定される「怖い話」と先回り対策

![SAI9.とるも眠れなくなるような問題](/images/LLM/SAI9.とるも眠れなくなるような問題.jpg)

### このページの要点

このページは、導入時に起こりやすいトラブルを **先に言語化して潰す** ページです。  
非常に実務的で、プロジェクトの信頼性を上げるページです。

### 想定される代表的な「怖い話」

#### 1. 「手元では動くのに本番で動かない」
原因例：
- イメージ差異
- ライブラリ差異
- CUDA/ドライバ差異
- 環境変数不足
- 証明書/通信制約

対策：
- コンテナ化
- イメージ固定（タグよりDigest推奨）
- 搬入ルール標準化
- 構成をYAML/Gitで管理

---

#### 2. GPU資源が足りない / 推論が遅い
原因例：
- モデルサイズ過大
- 同時接続数増加
- LoRA同時ロード過多
- バッチサイズ/並列設定不適合

対策：
- vLLM設定最適化（同時実行/メモリ設定）
- 監視導入（GPU使用率、TTFT、TPS）
- ベンチマークの標準化
- スケール設計（台数/ルーティング）

---

#### 3. 品質劣化（モデル・LoRAのドリフト）
原因例：
- ベースモデル更新
- LoRA差し替え
- 入力データ分布変化
- プロンプト変更

対策：
- 評価セット固定
- 定期評価
- 運用メトリクス監視
- リリース承認フロー

### 詳細解説（なぜ「怖い話」を先に出すのか）

エンタープライズでは、成功事例だけを並べるより、**失敗パターンと対策** を提示した方が信用されます。  
このページは、技術提案を「現場の言葉」に翻訳しているページです。

### 用語解説

- **TTFT (Time To First Token)**  
  最初の1トークンが返るまでの時間。体感性能に効く。
- **TPS (Tokens Per Second)**  
  1秒あたり生成トークン数。生成スループットの目安。
- **ドリフト**  
  運用中に入力や期待出力の性質が変わり、精度が落ちる現象。

### 実務Tips

- 「怖い話」ページは、対策をセットで書くのが鉄則。  
  問題だけ書くと不安を増やし、対策だけ書くと現実感がなくなります。
- ここで挙げた項目は、そのまま **受入試験項目** に落とし込めます。

---

## 第10章 4フェーズ導入ロードマップ

![SAI10.期間を見極める](/images/LLM/SAI10.期間を見極める.jpg)

### このページの要点

このページは、導入を一気にやらずに **段階的に進める計画** を示しています。  
スライドの中心メッセージは「最初から完璧を目指さない」です。

### フェーズ別の読み方（実務向け再整理）

> ※ スライドの表現を、現場で運用しやすい形に噛み砕いて再整理しています。

### Phase 1：基盤構築（環境を作る）
主な目的：
- OpenShift / OpenShift AI の利用準備
- GPU・ストレージ・ネットワークの疎通確認
- 搬入ルール（イメージ/モデル）整備

主な成果物：
- 利用可能Namespace/Project
- MinIOバケット
- レジストリ搬入手順
- 初期監視/ログ確認手順

---

### Phase 2：ベースモデル稼働（まず1本動かす）
主な目的：
- ベースモデル単体での推論成功
- レイテンシ/メモリ/安定性の基礎測定
- API化・利用側接続確認

主な成果物：
- 推論Serving定義（YAML）
- 動作確認レポート
- 初期ベンチマーク値

---

### Phase 3：Multi-LoRA化（価値を出す）
主な目的：
- 複数LoRAの登録/配布/切替
- ユースケース別のAI振る舞い提供
- LoRA運用ルール整備（命名/版管理/承認）

主な成果物：
- LoRA配置規約
- 切替手順
- LoRA別評価結果

---

### Phase 4：MLOps/運用高度化（止まらない仕組みへ）
主な目的：
- 監視/評価/ロールバック/承認フローの定着
- 運用手順書・障害対応手順の整備
- 継続改善サイクル構築

主な成果物：
- 運用Runbook
- 監視ダッシュボード
- リリース/切戻し手順
- 品質評価基準

### 詳細解説

この4フェーズは、技術だけでなく組織成熟度に合わせるための設計です。  
特に重要なのは **Phase 2とPhase 3を混ぜないこと** です。

- Phase 2：まずベースモデルで動くことを証明
- Phase 3：その上でLoRAを追加し価値を増やす

混ぜると、問題発生時に原因が分からなくなります（基盤問題なのかLoRA問題なのか不明）。

### 用語解説

- **MLOps**  
  機械学習/AIシステムを継続運用するための開発運用手法（監視、再学習、評価、リリース管理など）。
- **Runbook**  
  運用時の手順書。障害対応や定常作業の手順を定義する。
- **ロールバック**  
  変更後に問題が起きた際、以前の正常状態へ戻すこと。

### 実務Tips

- 各フェーズ終了時に「デモ可能な状態」を作ると、合意形成が進みます。
- 期間見積りは、技術工数だけでなく **申請・調整・承認工数** を必ず含めること。

---

## 第11章 現実的なトレードオフ設計：何を守り、何を諦めるか

![SAI11.何をあきらめるのか](/images/LLM/SAI11.何をあきらめるのか.jpg)

### このページの要点

このページは、プロジェクト成功の鍵である **トレードオフの明示** を扱っています。

典型的には、以下の軸のバランスを取ります。

- セキュリティ
- コスト（予算）
- 性能（速度・スループット）
- スコープ（やることの範囲）
- 納期/時間

### 詳細解説

主権AI基盤は、どれか1つを最大化すると他が苦しくなることが多いです。

#### 例1：セキュリティ最優先
- 搬入審査・承認を厳格化
- メリット：安全性高い
- デメリット：立ち上がりが遅くなる

#### 例2：性能最優先
- GPU増設 / 高性能GPU採用 / 余裕設計
- メリット：快適
- デメリット：予算増

#### 例3：スコープ最優先
- RAG / Multi-LoRA / MLOps / 監視を全部初回でやる
- メリット：完成度高く見える
- デメリット：納期遅延・品質低下リスク

このページの意図は、
> **「何を捨てるか」を先に決める勇気**
を持つことです。

### 実務での使い方（会議で有効）

このページは、要件対立の調整に使えます。

- 「全部欲しい」要求が出たとき
- 予算/納期が厳しいとき
- セキュリティ部門と開発部門の認識差があるとき

図を見せながら、どの軸を優先するかを合意します。

### 用語解説

- **トレードオフ**  
  ある価値を上げると、別の価値が下がる関係。
- **非機能要件**  
  機能そのものではなく、性能・可用性・セキュリティ・運用性などの要件。
- **可用性**  
  システムが必要なときに使える度合い。

### 実務Tips

- トレードオフは議事録に残すこと。  
  後で「なぜこの判断になったのか」の説明責任に効きます。
- 「今回は捨てるが次フェーズで回収する」を明文化すると、関係者の納得感が高まります。

---

## 第12章 導入要件整理（ハードウェア / ソフトウェア / 人的スキル）

![SAI12.何をどれだけ必要なのか](/images/LLM/SAI12.何をどれだけ必要なのか.jpg)

### このページの要点

このページは、導入時の「必要条件」を整理するページです。  
技術PoCより前に、**何が足りないか** を洗い出すために使います。

---

### 1. ハードウェア要件（例）

#### GPU
- 推論用GPU（例：H100/A100/L40S/RTX系など構成に応じて）
- Multi-LoRAや同時接続数を考えると、メモリ容量が重要

#### CPU / メモリ
- OpenShiftワーカーノード
- 推論補助処理・ルーティング・監視・ログ基盤向けのCPU/メモリ

#### ストレージ
- ベースモデル保管（容量大）
- LoRA/評価データ/ログ保管
- バックアップ領域

#### ネットワーク
- エアギャップ搬入経路
- 内部通信帯域
- DNS/証明書/名前解決

---

### 2. ソフトウェア要件（例）

- OpenShift / OpenShift AI
- KServe / vLLM（推論）
- MinIO（S3互換ストレージ）
- コンテナレジストリ（Quay等）
- 監視（Prometheus/Grafana）
- ログ/監査基盤
- Git / GitOps / CI/CD（Argo CD/Tekton等、環境に応じて）

---

### 3. 人的スキル要件（例）

#### 基盤系
- Kubernetes / OpenShift運用
- コンテナ/レジストリ
- ネットワーク/証明書/セキュリティ

#### AI系
- LLM推論（vLLM/KServe）
- LoRA/PEFTの基本理解
- 評価指標（精度/TTFT/TPS等）

#### 運用系
- 監視/ログ分析
- 障害対応
- 変更管理・リリース管理

### 詳細解説

このページのポイントは、**「技術スタック」だけでなく「人のスキル」も要件として明示していること** です。  
AI基盤案件で実際に不足しがちなのは、次の「つなぎ役」です。

- 基盤担当とAI担当の会話をつなぐ人
- セキュリティ要件を実装に落とす人
- 業務要求をLoRA/評価指標に翻訳する人

### 用語解説

- **GPUメモリ（VRAM）**  
  GPU上のメモリ。LLM推論可能なモデルサイズや同時実行性に大きく影響。
- **GitOps**  
  宣言的設定（YAML等）をGitで管理し、環境へ反映・同期する運用手法。
- **Tekton / Argo CD**  
  Kubernetes上でCI/CDやデプロイ自動化を行うための代表的ツール。

### 実務Tips

- 要件表は「ある/ない」だけでなく、**誰が管理しているか** も併記すると実行計画に落とし込みやすいです。
- 人的スキル要件は、外部支援/育成計画とセットで書くと現実的になります。

---

## 第13章 まとめと次のアクション

![SAI13.NextActions＆Summary](/images/LLM/SAI13.NextActions＆Summary.jpg)

### このページの要点

最終ページは、単なる締めではなく **「次に何をするか」** を明確にするページです。  
技術資料としてとても重要です。

### このページが果たす役割

- 資料全体のメッセージを再確認
- 合意してほしいポイントを再提示
- 具体的な次アクションを箇条書きで示す

### 典型的な次アクション（実務向け再整理）

#### 直近（今週〜来週）
- PoC対象ユースケースを1つ決める
- 関係者（基盤/セキュリティ/利用部門）を確定
- 使えるGPU・Namespace・ストレージ有無を確認

#### 短期（今月）
- ベースモデル単体推論を成功させる
- LoRA運用方針（命名/版管理/承認）を決める
- 搬入/配布/切戻しの手順ドラフトを作る

#### 中期
- Multi-LoRA化
- 評価/監視導入
- 運用Runbook整備
---
# （まとめ）プロベートAIキッチン：インセプションデッキ
![SAI14.（まとめ）プロベートAIキッチン：インセプションデッキ](/images/LLM/SAI14.（まとめ）プロベートAIキッチン：インセプションデッキ.jpg)

---

# 付録A：図に出てくる重要用語集（初学者向け）

## 主権AI（Sovereign AI）
自組織のルール・責任・インフラの管理下で運用するAIの考え方。  
「全部内製」ではなく、**統制可能性を持つこと** が本質。

## エアギャップ（Air Gap）
外部ネットワークと隔離された環境。  
セキュリティは高いが、配布・更新・運用の工夫が必要。

## OpenShift / OpenShift AI
- OpenShift：企業向けKubernetes基盤
- OpenShift AI：その上でAI/MLの実行・管理をしやすくする機能群

## KServe
Kubernetes上に推論APIをデプロイするための仕組み。  
推論サービスの標準化に使いやすい。

## vLLM
LLM推論を高速化するエンジン。  
複数リクエストの効率処理やメモリ活用に強みがある。

## MinIO（S3互換）
オブジェクトストレージ。  
LoRAや評価データ、モデル配布アーティファクトの保管先に適する。

## LoRA / アダプタ
ベースモデルに対する差分学習ファイル。  
用途別の振る舞いを追加しやすい。

## ベースモデル
土台となるLLM本体。  
サイズが大きく、保管・起動・GPUメモリ設計の中心になる。

## Multi-LoRA
1つのベースモデルに対して複数LoRAを運用するパターン。  
効率的だが、評価・運用ルールの設計が重要。

---

# 付録B：実務でよく使うコマンド集（図にはないが重要）

> ここでは、OpenShift AI / エアギャップ運用 / モデル配布でよく使うコマンドを、初学者向けに丁寧に解説します。  
> 実際の環境名（project名、URL、レジストリ名）は各現場に合わせて変更してください。

---

## B-1. OpenShift / Kubernetes 基本確認コマンド（`oc`）

### 1) ログイン中ユーザー確認
```bash
oc whoami
````

* **何をするコマンド？**
  現在どのユーザーでOpenShiftにログインしているかを確認します。
* **よくある用途**
  権限不足エラーが出たときの第一確認。

---

### 2) プロジェクト（Namespace）一覧確認

```bash
oc projects
```

* **何をするコマンド？**
  自分がアクセス可能なProject一覧を表示します。
* **補足**
  OpenShiftではProject = KubernetesのNamespaceを扱いやすくした概念。

---

### 3) 作業対象のProjectへ切替

```bash
oc project my-ai-project
```

* **何をするコマンド？**
  以降の`oc`操作対象を`my-ai-project`へ切り替えます。
* **実務Tips**
  誤Projectで`apply`しないよう、作業前に毎回実行すると安全です。

---

### 4) Pod一覧確認

```bash
oc get pods -n my-ai-project
```

* **`get`**: リソース一覧取得
* **`pods`**: Pod（コンテナ実行単位）
* **`-n`**: Namespace/Project指定

#### よく見る状態

* `Running`：稼働中
* `Pending`：起動待ち（GPU不足/スケジューリング不可の可能性）
* `CrashLoopBackOff`：起動→失敗を繰り返し

---

### 5) Podの詳細確認（障害調査で重要）

```bash
oc describe pod <pod名> -n my-ai-project
```

* **何を見る？**

  * イベント（Event）
  * イメージPull失敗
  * ボリュームMount失敗
  * GPUスケジューリング失敗
* **用途**
  `Runningにならない` ときの原因特定。

---

### 6) ログ確認（推論サーバ調査の基本）

```bash
oc logs -f <pod名> -n my-ai-project
```

* **`logs`**: コンテナログ表示
* **`-f`**: follow（追尾表示）
* **用途**
  vLLM/KServeの起動エラー、モデルロード失敗、LoRAロード失敗の調査。

---

### 7) YAML適用（デプロイ反映）

```bash
oc apply -f serving.yaml
```

* **何をする？**
  YAML定義をクラスタへ反映します（作成/更新）。
* **実務Tips**
  `apply`前にGit差分確認、`project`確認を習慣化すると事故が減ります。

---

### 8) デプロイ反映状況確認（ロールアウト）

```bash
oc rollout status deployment/my-serving -n my-ai-project
```

* **何をする？**
  デプロイ更新が完了したか確認。
* **用途**
  `applyしたけど反映終わった？` の確認。

---

### 9) リソース使用量確認（環境により有効）

```bash
oc top pods -n my-ai-project
oc top nodes
```

* **何を見る？**
  CPU/メモリ使用量。
  （GPUは別ツールやExporter連携で見ることが多い）
* **注意**
  Metrics Server等が入っていないと使えないことがあります。

---

## B-2. コンテナイメージ搬入・管理（エアギャップで重要）

### 1) イメージ取得（接続可能環境側）

```bash
podman pull registry.example.com/my-llm-serving:1.0.0
```

* **用途**
  搬入前にイメージを取得。

---

### 2) イメージ保存（tar化）

```bash
podman save -o my-llm-serving_1.0.0.tar registry.example.com/my-llm-serving:1.0.0
```

* **何をする？**
  コンテナイメージをファイル化（搬入しやすくする）。
* **実務Tips**
  バージョン・日付をファイル名に含めると管理しやすい。

---

### 3) チェックサム作成（改ざん/破損確認）

```bash
sha256sum my-llm-serving_1.0.0.tar > my-llm-serving_1.0.0.tar.sha256
```

* **何をする？**
  ハッシュ値（指紋）を作成。
* **用途**
  搬入前後で同一ファイルか検証。

---

### 4) イメージ読込（閉域側）

```bash
podman load -i my-llm-serving_1.0.0.tar
```

* **何をする？**
  tarからローカルのコンテナストアへ読み込む。

---

### 5) レジストリへPush（閉域側レジストリ）

```bash
podman login image-registry.example.local
podman tag registry.example.com/my-llm-serving:1.0.0 image-registry.example.local/ai/my-llm-serving:1.0.0
podman push image-registry.example.local/ai/my-llm-serving:1.0.0
```

* **何をする？**
  閉域側のレジストリへ再登録。
* **用途**
  OpenShiftからPull可能にするため。

---

## B-3. MinIO / S3（モデル・LoRA保管）

> MinIO Client（`mc`）が使える環境の例です。

### 1) 接続先設定

```bash
mc alias set localminio https://minio.example.local ACCESS_KEY SECRET_KEY
```

* **`alias set`**: 接続先に名前を付ける
* **用途**
  毎回URLやキーを書かずに操作できるようにする。

---

### 2) バケット作成

```bash
mc mb localminio/models
mc mb localminio/lora
```

* **`mb`**: make bucket
* **用途**
  モデル格納先の準備。

---

### 3) ファイル配置（LoRAアップロード）

```bash
mc cp -r ./my-lora/ localminio/lora/helpdesk-classifier/v1/
```

* **`cp -r`**: ディレクトリごとコピー
* **用途**
  `adapter_config.json`, `adapter_model.safetensors` をまとめて配置。

---

### 4) 配置確認

```bash
mc ls localminio/lora/helpdesk-classifier/v1/
mc stat localminio/lora/helpdesk-classifier/v1/adapter_model.safetensors
```

* **`ls`**: 一覧
* **`stat`**: サイズ/更新日時など詳細
* **用途**
  配布漏れや破損の早期発見。

---

## B-4. API疎通確認（`curl`）

### 1) 推論APIの基本疎通確認（GET）

```bash
curl -v https://inference.example.local/healthz
```

* **`-v`**: 詳細表示（通信の流れ、TLS交渉など）
* **用途**
  まず入口が生きているか確認。

---

### 2) 自己署名/社内CA環境での確認（暫定）

```bash
curl -vk https://inference.example.local/healthz
```

* **`-k`**: 証明書検証をスキップ（検証用）
* **注意**
  恒久運用では非推奨。必ずCA証明書設定に寄せること。

---

### 3) 推論リクエスト（POSTの例）

```bash
curl -sS -X POST https://inference.example.local/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "base-with-lora-helpdesk",
    "messages": [
      {"role": "user", "content": "問い合わせ文を3行で要約してください"}
    ]
  }'
```

#### オプション解説

* **`-sS`**: 通常は静かに、エラー時は表示
* **`-X POST`**: HTTPメソッド指定
* **`-H`**: ヘッダ指定
* **`-d`**: リクエストボディ（JSON）

---

### 4) 応答時間測定（運用で便利）

```bash
curl -s -o /dev/null -w 'time_total=%{time_total}\nhttp_code=%{http_code}\n' \
  https://inference.example.local/healthz
```

* **`-o /dev/null`**: 本文捨てる
* **`-w`**: 指定フォーマットで結果表示
* **用途**
  ヘルスチェックの定点観測。

---

## B-5. TLS/証明書確認（`openssl`）

### 1) サーバ証明書チェーン確認

```bash
openssl s_client -connect inference.example.local:443 -showcerts
```

* **用途**
  証明書チェーンやCN/SAN確認、ハンドシェイク調査。
* **よくある使い道**
  `curl`のTLSエラー時の深掘り。

---

### 2) 証明書内容を読む（ローカルファイル）

```bash
openssl x509 -in company-ca.crt -text -noout
```

* **何を見る？**

  * 有効期限
  * 発行者（Issuer）
  * Subject
  * SAN（Subject Alternative Name）

---

## B-6. JSON整形（`jq`）

### 1) 応答JSONの整形表示

```bash
curl -s https://inference.example.local/metrics-json | jq .
```

* **何をする？**
  JSONを見やすく整形。
* **実務Tips**
  APIデバッグ時の必須級ツール。

### 2) 特定項目だけ抜く

```bash
curl -s https://inference.example.local/v1/models | jq '.data[].id'
```

* **用途**
  登録モデル一覧の確認など。

---

## B-7. ログ/ファイル調査の基本（Linux系）

### 1) 文字列検索（ログ調査）

```bash
grep -i "error" app.log
```

* **`-i`**: 大文字小文字無視
* **用途**
  エラー行の抽出。

### 2) 再帰検索（設定/ソース全体）

```bash
grep -R "max-loras" .
```

* **用途**
  どのYAML/設定で指定しているか確認。

### 3) 高速検索（ripgrepがある場合）

```bash
rg "lora|adapter" .
```

* **用途**
  設定やコードの素早い横断検索。

---

## B-8. よくある確認順（障害対応の型）

LLM推論が動かない時の、現場で使いやすい順番です。

1. `oc project ...`（対象Project確認）
2. `oc get pods`（Pod状態確認）
3. `oc describe pod ...`（イベント確認）
4. `oc logs -f ...`（推論サーバログ確認）
5. `curl -vk .../healthz`（入口疎通確認）
6. `mc ls ...`（LoRA/モデル配置確認）
7. 設定YAMLの差分確認（Git diff）

この順で見ると、切り分けが速くなります。

