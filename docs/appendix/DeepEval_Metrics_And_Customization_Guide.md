# DeepEval：Metrics & Customization Guide

この資料は **DeepEval（LLM評価フレームワーク）** の「代表的メトリクス」と「カスタマイズ方法」を、**RAG/Agent/Safety** の観点で整理したガイドです。
各ページ（=スライド）をそのまま VitePress に差し込めるよう、**画像→要旨→用語→Tips** の順にまとめます。

## 1. DeepEval：Metrics & Customization Guide（全体像）

![Dev1.DeepEval実践ガイド：メトリックスの詳細とカストマイズ](/images/IaC/Dev1.DeepEval実践ガイド：メトリックスの詳細とカストマイズ.jpg)


### 図の要旨
- 資料の目的は「LLMアプリの品質を **測れる形** にする」こと（作ったら終わりではなく、継続的に改善できる状態へ）。
- DeepEval は **LLM-as-a-Judge（LLMによる評価）** と **決定的なルール評価** を組み合わせられる。
- 対象は主に RAG（検索＋生成）、Agent（ツール実行）、Safety（安全性）など、LLMアプリの主要ユースケース。

### 詳細解説（用語）
- **DeepEval**：LLMアプリの評価（Evals）をテストの形で書けるOSS。pytestと相性が良い。
- **Metric（メトリクス）**：評価基準。例：回答が質問に関係しているか（Answer Relevancy）、根拠に忠実か（Faithfulness）など。
- **Customization（カスタマイズ）**：業務ドメインに合わせて評価軸・評価プロンプト・判定ロジックを拡張すること。

### Tips（実務）
- まずは「何を良い回答と呼ぶか」を言語化（品質基準の言葉を揃える）→ それをメトリクスに落とす、の順が最短です。
- 最初から完璧を狙わず、**代表ユースケース3〜5本** のテストを作って回帰（Regression）できる状態を作ると伸びが速いです。

---

## 2. 評価はユニットテストへ：生成AI評価のパラダイムシフト

![Dev2.LLM評価のパラダイムシフト：ユニットテストとしての評価](/images/IaC/Dev2.LLM評価のパラダイムシフト：ユニットテストとしての評価.jpg)

### 図の要旨
- 従来のルールベース評価（BLEU/ROUGE等）では、LLMアプリの品質を捉えにくい → **評価もLLMで** という流れ。
- 評価を「データ分析」ではなく「**ユニットテスト**」として管理し、CIで回す（壊れたら検知できる）。
- LLMアプリの不具合は、コードだけでなく **プロンプト・知識・モデル** でも発生するため、テストの粒度を変える必要がある。

### 詳細解説（用語）
- **LLM-as-a-Judge**：別のLLMを“審判”として使い、回答の品質を採点・判定する方式。
- **ユニットテスト化**：`assert score >= threshold` のように、品質を“落第/合格”できる形にする。
- **フレーク（Flaky）**：同じ入力でも判定が揺れる状態。評価プロンプト・モデル選定・温度設定で対策する。

### Tips（実務）
- CIに載せる前に、まずローカルで「1回で通る」ではなく **10回回して安定** を見るのがコツです。
- 評価LLMは **温度=0**・出力フォーマット固定（JSON等）・採点基準を明文化、が安定化の基本です。

---

## 3. DeepEvalメトリクスの全体像：RAG / Agentic / Safety & Quality / Custom

![Dev3.DeepEvalメトリクス＆タキソノミー](/images/IaC/Dev3.DeepEvalメトリクス＆タキソノミー.jpg)


### 図の要旨
- DeepEvalのメトリクスは大きく **RAG / Agentic / Safety&Quality / Custom** に分けられる。
- RAGはさらに **Retriever（検索）** と **Generator（生成）** に分割して評価するのがポイント。
- 足りない評価軸は、G-EvalやDAG（決定木/グラフ）で拡張できる。

### 詳細解説（用語）
- **Retriever / Generator**：RAGを「検索で何を持ってくるか」と「それを使ってどう答えるか」に分ける考え方。
- **Safety & Quality**：有害性（Toxicity）や偏見（Bias）、幻覚（Hallucination）など“出してはいけない”を評価する軸。
- **Custom Metrics**：業務固有の品質（例：用語統一、規約遵守、JSONスキーマ遵守）を測るための拡張。

### Tips（実務）
- RAGの改善は「検索が悪いのか、生成が悪いのか」で打ち手が変わります。まずは **Retriever/Generatorを別テストに** してください。
- 最初の導入は、RAGなら「Answer Relevancy」「Faithfulness」「Contextual Precision/Recall」の4本で十分戦えます。

---

## 4. RAG（Generator）評価：Answer Relevancy と Faithfulness

![Dev4.RAG評価（１）：Generator（生成）の評価](/images/IaC/Dev4.RAG評価（１）：Generator（生成）の評価.jpg)


### 図の要旨
- Generator評価の代表は **Answer Relevancy**（質問に答えているか）と **Faithfulness**（根拠から逸脱していないか）。
- Faithfulnessの評価では、回答を“主張”に分解し、コンテキストで裏取りする（QAG的な発想）。
- RAGの品質事故（それっぽい嘘）は、Faithfulnessでかなり早期に検知できる。

### 詳細解説（用語）
- **Answer Relevancy**：質問と回答の意味的近さ。雑談・脱線・言い換え過多を検知しやすい。
- **Faithfulness（忠実性）**：回答が与えられた根拠（コンテキスト）に支持されているか。
- **QAG（Question-Answer Generation）**：回答やコンテキストから質問/QAを生成して整合を取る系の評価テクニック。

### Tips（実務）
- 社内ドキュメントRAGなら「根拠が必要な質問」ほどFaithfulnessが効きます（規程・手順・数値など）。
- Faithfulnessのテストデータは、最初は **“誤りが出やすい質問”**（曖昧・例外が多い）を優先すると投資対効果が高いです。

---

## 5. RAG（Retriever）評価：Contextual Precision / Recall / Relevancy

![Dev5.RAG評価（２）：Retriever（検索）の評価](/images/IaC/Dev5.RAG評価（２）：Retriever（検索）の評価.jpg)

### 図の要旨
- Retriever評価は「必要な情報を取れているか」「ノイズを混ぜていないか」を測る。
- 代表は **Contextual Precision（精度）** と **Contextual Recall（再現率）**、そして **Contextual Relevancy**。
- Generatorが良くてもRetrieverが悪いと破綻するため、RAGは必ずRetriever側も評価する。

### 詳細解説（用語）
- **Contextual Precision**：取得したコンテキストのうち“役に立つ部分”の割合（ノイズが少ないほど高い）。
- **Contextual Recall**：必要な情報を取りこぼしていないか（必要情報をカバーできているほど高い）。
- **Contextual Relevancy**：質問に対してコンテキストが関連しているか（類似度だけでなく意味の関連）。

### Tips（実務）
- Precisionが低い→ chunkが大きすぎ/TopKが多すぎ/検索クエリが広すぎ、を疑う。Recallが低い→ chunkが小さすぎ/TopKが少なすぎ/Embeddingや索引を疑う。
- Retrieverのテストは「期待する根拠がどの文書にあるか」を **ざっくりでもメモ** しておくと、改善が回しやすいです。

---

## 6. Agentic & Safety/Quality 評価：ツール実行と有害性のチェック

![Dev6.エージェント機能と安全性の評価](/images/IaC/Dev6.エージェント機能と安全性の評価.jpg)


### 図の要旨
- Agentic（ツール実行）では「正しいツール選択」「正しい引数」「正しい順序」が品質になる。
- Safety/Qualityでは **Toxicity / Bias / Hallucination / Prompt Injection耐性** などの評価が重要。
- 多くのメトリクスは **threshold（合格点）** を設けて“落第させる”運用が想定。

### 詳細解説（用語）
- **Agentic**：LLMが外部ツール（API/DB/ブラウザ等）を呼び出してタスクを進める方式。
- **Tool correctness**：呼び出すべきツールが選ばれたか、引数が妥当か、結果を次ステップに正しく渡せたか。
- **Safety metrics**：有害性、個人情報、違反コンテンツ、注入攻撃への脆弱性などを測る評価軸。

### Tips（実務）
- Agentは「成功/失敗」だけだと改善点が見えません。**tool選択 / 引数 / 期待出力** を分けてメトリクス化するとデバッグが楽です。
- Safetyは“全部を毎PRでやる”より、**軽量（毎PR）＋重い（夜間/週次）** の二段構えが現実的です。

---

## 7. カスタマイズ戦略①：G-Eval（LLMベースの汎用・業務品質評価）

![Dev7.カスタマイズ戦略①：G-Eval（評価のゴールドスタンダード）](/images/IaC/Dev7.カスタマイズ戦略①：G-Eval（評価のゴールドスタンダード）.jpg)

### 図の要旨
- G-Evalは、採点基準（criteria）を与えてLLMに評価させる **汎用カスタムメトリクス**。
- スコアリングのプロンプトに CoT（推論）を使い、最後にスコアだけを抽出する設計が多い。
- strict mode（厳格判定）や出力形式の固定で、評価の揺れを減らす。

### 詳細解説（用語）
- **G-Eval**：基準文（criteria）＋評価対象（input/actual/expected等）を渡し、0-1や1-5などで採点させる仕組み。
- **criteria**：『用語が統一されている』『手順が抜けていない』『禁止事項を含まない』など、業務品質の文章化。
- **strict_mode**：曖昧な場合に甘く合格させない（守りを固める）ための設定概念。

### Tips（実務）
- まずは criteria を **3〜5個に絞る**（増やすほど評価が不安定になりやすい）。足りない時に増やす。
- 社内向けなら「敬語/トーン」「規約語彙」「手順の抜け漏れ」をG-Evalにすると効きやすいです。

---

## 8. カスタマイズ戦略②：DAG & Deterministic（決定的ルール評価）

![Dev8.カスタマイズ戦略②：DAGと決定論的ンメトリクス](/images/IaC/Dev8.カスタマイズ戦略②：DAGと決定論的ンメトリクス.jpg)


### 図の要旨
- LLM評価だけだと揺れる/高コスト → できるところは **決定的（Deterministic）** に評価する。
- DAG（有向グラフ）や決定木で、複数条件の判定を組み合わせる（例：形式→必須項目→値域）。
- 「まず機械で弾けるものを弾き、残りをLLMで見る」が強い。

### 詳細解説（用語）
- **Deterministic metric**：正規表現、JSON schema、数値比較など、同じ入力なら必ず同じ結果になる評価。
- **DAG（Directed Acyclic Graph）**：評価をノードに分解し、依存順に流す設計。複合条件の判定に向く。
- **ハイブリッド評価**：決定的チェック（高速）→ LLM評価（意味）→ 人手レビュー（重要ケース）と段階化。

### Tips（実務）
- 例：『出力はJSONで、必須キーが揃い、日付はISO8601』は決定的に落とせます。意味の妥当性だけをLLMに任せるとコスパが上がります。
- CIではまず決定的評価を毎回、LLM評価は差分が大きい時だけ、などの運用も有効です。

---

## 9. カスタマイズ戦略③：Evaluation Template（評価プロンプトの上書き）

![Dev9.カスタマイズ戦略③：評価プロンプトのオーバーライド](/images/IaC/Dev9.カスタマイズ戦略③：評価プロンプトのオーバーライド.jpg)


### 図の要旨
- 既存メトリクスでも、評価プロンプト（evaluation template）を差し替えることで **ドメイン適合** できる。
- 言語（日本語）・業界用語・禁止事項・採点観点をテンプレートに埋め込む。
- few-shot（例示）を入れると安定することがあるが、入れすぎると過学習/コスト増にもなる。

### 詳細解説（用語）
- **Evaluation Template**：評価LLMに渡す“採点指示書”。多くの揺れはここで改善できる。
- **few-shot**：良い/悪い例を数件見せて評価の軸を固定する手法。
- **domain vocabulary**：業務固有の用語。評価側が理解していないと誤判定が増えるため、辞書的に補助する。

### Tips（実務）
- 日本語運用ならテンプレに『日本語で、曖昧な場合は減点』『敬語・社内用語を優先』を明記すると安定します。
- テンプレ変更は“評価の仕様変更”なので、変更前後で同じテストセットを流して差分を確認（回帰）してください。

---

## 10. LLM-as-a-Judge を安定させる：評価モデル選択とフレーク対策

![Dev10.LLM-as-a-Judgeの仕組みとモデル選択](/images/IaC/Dev10.LLM-as-a-Judgeの仕組みとモデル選択.jpg)


### 図の要旨
- LLM-as-a-Judge は強力だが、モデルや設定次第で評価が揺れる（フレーク）。
- 対策は「評価対象の分解」「出力フォーマット固定」「温度0」「同一モデル固定」「再試行/多数決」など。
- DeepEvalは複数プロバイダ（OpenAI/Azure/Bedrock/Ollama等）を差し替えられる設計。

### 詳細解説（用語）
- **Judge model**：評価用に使うLLM。生成モデルと同じである必要はない（安定重視で選ぶ）。
- **Atomization（分解）**：1つの評価で多くを見ない。『関連性』『忠実性』『形式』などに分けるほど安定する。
- **Provider**：OpenAI/Azure/Bedrock/Ollama等。社内制約（データ持ち出し）で選択肢が決まることがある。

### Tips（実務）
- 評価モデルは **本番モデルと別** にしてもOKです（むしろ、安定で安価なモデルに寄せると運用しやすい）。
- 揺れが出たら「基準が曖昧」か「1回で判定している」ことが多いです。**“減点条件”を明文化** すると改善します。

---

## 11. pytest連携：LLM評価をCIに組み込む

![Dev11.開発ワークフローへの統合：Pytest](/images/IaC/Dev11.開発ワークフローへの統合：Pytest.jpg)


### 図の要旨
- DeepEvalはpytestと合わせて「LLMテスト」を書ける（例：`assert_test()`）。
- テストは dataset を回してスコアを集計し、threshold未満をFAILにできる。
- 並列実行（pytest-xdist）やキャッシュで、実行時間を現実的にできる。

### 詳細解説（用語）
- **assert_test**：DeepEval側のテストヘルパ。メトリクス・閾値・テストケースをまとめて評価する発想。
- **dataset-driven test**：テストケースをデータで管理し、同じ評価を大量に回す方式（回帰に強い）。
- **xdist**：pytestの並列実行プラグイン。LLM評価は待ちが長いので効果が出やすい。

### Tips（実務）
- CIに載せる最初の形は「軽量セット（10〜30ケース）」がおすすめです。重いセットは夜間ジョブに。
- pytestで落ちた時に原因が追えるよう、**テストケースにID・期待根拠・メモ** を持たせると運用が回ります。

---

## 12. Confident AI で運用：可観測性・回帰テスト・比較

![Dev12.ConfidentAI：評価プラットフォームの活用](/images/IaC/Dev12.ConfidentAI：評価プラットフォームの活用.jpg)


### 図の要旨
- Confident AI（プラットフォーム）と組み合わせると、評価結果の蓄積・可視化・比較がしやすい。
- 回帰テスト（Regression）やA/B比較、プロンプト変更の影響分析に向く。
- “手元のテスト”から“運用品質管理”へ広げる位置づけ。

### 詳細解説（用語）
- **Observability**：本番ログや評価結果を観測して、劣化を検知する仕組み。
- **Regression testing**：プロンプト/モデル/検索設定を変えた時に、品質が落ちていないかを継続検証すること。
- **Comparison**：バージョンA/Bで同じテストセットを回し、どこが良くなった/悪くなったを比較する。

### Tips（実務）
- まずはローカルpytestで“合否”が安定してから、プラットフォーム導入を検討すると失敗しにくいです。
- 評価のKPI（例：Faithfulness平均0.85以上、Toxicity 0件）を決めると、改善サイクルが回ります。

---

## 13. ベストプラクティス：The Rule of 5（メトリクスを絞る）

![Dev13.ベストプラクティス：効果的な評価戦略](/images/IaC/Dev13.ベストプラクティス：効果的な評価戦略.jpg)


### 図の要旨
- メトリクスは増やしすぎると運用不能になるため、**Rule of 5（だいたい5本）** に絞る指針。
- 汎用メトリクス2〜3本 + ドメイン固有1〜2本、が現実的な最小構成。
- “reference-free（正解文なし）”評価も活用できるが、基準の言語化が重要。

### 詳細解説（用語）
- **Rule of 5**：品質指標を5つ前後に絞る経験則。『測れるけど改善できない』指標を増やさない。
- **Generic metrics**：一般的に効く評価軸（関連性、忠実性、有害性など）。
- **Domain metrics**：業務特有の品質（形式、規約、用語、法令、社内手順など）。

### Tips（実務）
- 最初のおすすめ5本：Answer Relevancy / Faithfulness / Contextual Precision / Contextual Recall /（ドメイン1本）。
- ドメイン1本は「出力フォーマット（JSON）」「禁止語」「手順抜け漏れ」など、**炎上しやすい失敗** から選ぶと効きます。

---

## 14. 信頼性の高いLLMアプリへ：Unit Testing / Customization / Lifecycle

![Dev14.信頼性の高いLLMアプリケーションへ](/images/IaC/Dev14.信頼性の高いLLMアプリケーションへ.jpg)


### 図の要旨
- 信頼性（Robust AI）は、(1)Unit Testing（テストとして管理）(2)Customization（自社品質へ）(3)Lifecycle（継続改善）の3本柱。
- “厳格に評価し、責任を持ってデプロイせよ”というメッセージで締める。
- LLMアプリは“仕様が揺れる”ため、テストと運用の両輪が必須。

### 詳細解説（用語）
- **Unit Testing**：LLMの振る舞いをテストで固定し、変更で壊れたら検知できるようにする。
- **Lifecycle**：開発→テスト→本番→監視→改善、のループを回すこと。
- **Responsible deployment**：安全性・品質・説明責任を満たした上でのリリース。

### Tips（実務）
- 運用のコツは「テスト（CI）で守る領域」と「監視（本番）で守る領域」を分けることです（例：安全は本番監視も必須）。
- “壊れ方”を先に想定して、重要ユースケースからテスト化すると、最小コストで効果が出ます。

---

## 15. リソースとドキュメント（公式リンク）

![Dev15.リソースとドキュメント](/images/IaC/Dev15.リソースとドキュメント.jpg)


### 図の要旨
- 公式Docs、GitHub、Confident AI の案内。
- 運用で詰まったら、まずはDocsのメトリクス詳細と、評価テンプレの例を参照するのが近道。
- OSSなので、Issue/PRでの情報も価値がある。

### 詳細解説（用語）
- **Docs**：メトリクス仕様・サンプル・プロバイダ設定など、一次情報。
- **GitHub repo**：コード・Issue・サンプル。バージョン差分や既知バグの確認に便利。
- **Confident AI**：評価の運用基盤（可視化・比較・継続改善）。

### Tips（実務）
- 実装で迷ったら、まずは「自分のユースケースに一番近いサンプル」を探して最小構成で動かすのが最速です。
- 評価LLMのコストが気になる場合は、(1)決定的チェック増やす (2)ケース数を層別（毎PR/夜間） (3)軽量judgeに寄せる、の順で効きます。

---
