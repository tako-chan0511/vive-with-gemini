## 1. 生成AI品質保証への戦略的フィールドガイド
![Gu1.生成AI品質保証への戦略的フィールドガイド](/images/IaC/Gu1.生成AI品質保証への戦略的フィールドガイド.jpg)

### 図の要旨
- 生成AI（LLM）を **「安全に・安定して・事業に耐える品質で」**　運用するための全体像（ガードレール／レッドチーミング／評価基盤／Kubernetes）を俯瞰する導入ページです。
- キーメッセージは **AI品質は“モデルの良し悪し”だけではなく、工程（仕様→テスト→運用）で作る** という考え方です。

### 詳細解説（用語）
- **ガードレール（Guardrails）**：入力・出力・運用の各段に「してよいこと／だめなこと」を明文化し、違反を検知・遮断・修正する仕組み。
- **レッドチーミング（Red Teaming）**：悪意ある入力（脱獄、注入、難読化など）を“わざと”投げ、弱点を先に見つけるテスト。
- **評価（Evaluations）**：正しさ・安全性・一貫性などを定量指標で測り、回帰（regression）として継続実行するテスト。

### Tips（実務）
- 最初に「品質の定義（仕様）」を1枚で作ると運用が楽になります（例：*禁止領域*、*引用必須*、*機密取り扱い*、*出力フォーマット*、*SLO*）。
- “モデル更新”より先に、**評価セット（代表質問＋禁止質問）**　を固定しておくと、改善が本物かどうか判定できます。

---

## 2. テストのパラダイムシフト：決定論的から確率的へ
![Gu2.テストのパラダイムシフト：決定論的から確率的へ](/images/IaC/Gu2.テストのパラダイムシフト：決定論的から確率的へ.jpg)

### 図の要旨
- 従来ソフトは「同じ入力→同じ出力」を前提に **決定論的テスト** が成立します。
- LLMは同じ入力でも揺れる（確率的）ため、テストは **“動作の監査（Audit）”** に近づきます（目的：*期待範囲に収まるか*）。

### 詳細解説（用語）
- **決定論（Deterministic）**：入力が同じなら結果が固定。典型は関数・API・DB。
- **確率的（Probabilistic）**：入力が同じでも結果が揺れる。LLMの生成は代表例。
- **監査（Audit）**：結果を“合否”の一点で見るのではなく、根拠・安全性・逸脱を観測して統制する。

### Tips（実務）
- LLMテストは「単発のexpected一致」より、**プロパティ（性質）** で設計すると強いです。  
  例：  
  - *機密情報を出さない*（PII/社内情報の漏えい禁止）  
  - *根拠のない断定をしない*（不明は不明と言う）  
  - *JSONを必ずschemaで返す*（構造の保証）
- 揺れ対策として、まずは **temperatureを0〜0.3** で評価 → 安定してから創造性を上げる、が堅い順序です。

---

## 3. AIガードレールの解剖学
![Gu3.AIガードレールの解剖学](/images/IaC/Gu3.AIガードレールの解剖学.jpg)

### 図の要旨
- ガードレールは大きく **入力側（Input Rail）** と **出力側（Output Rail）** に分かれます。
- 重要なのは、ガードレールを **モデルから独立した「ポリシー実行レイヤ」** として設計することです（モデルを変えても統制が残る）。

### 詳細解説（用語）
- **Input Rail**：ユーザー入力／コンテキスト（RAGの検索結果）を検査する。  
  例：プロンプト注入、脱獄、PII、スコープ外質問、悪性URL等
- **Output Rail**：生成結果を検査する。  
  例：ハルシネーション、毒性、差別、機密、フォーマット違反、引用欠落等
- **ポリシー（Policy）**：許可・禁止・必須条件を文章ではなく“機械が実行できる規則”に落としたもの。

### Tips（実務）
- 実装は「検知→対応」をセットにすると運用しやすいです。  
  - 検知：ルール／分類器／正規表現／LLM-as-a-judge  
  - 対応：ブロック、伏字、再生成、要約に切替、有人エスカレーション
- RAGの場合、**“入力（質問）”だけでなく“コンテキスト（検索結果）”も検査対象**にするのが効果的です（汚染データ／機密混入対策）。

---

## 4. 主要なガードレール実装フレームワーク
![Gu4.主要なガードレール実装フレームワーク](/images/IaC/Gu4.主要なガードレール実装フレームワーク.jpg)

### 図の要旨
- ガードレール実装には代表的に3系統があります。  
  1) **プログラマブル（会話フロー制御）**  
  2) **構造的（出力スキーマ保証）**  
  3) **モデルベース（安全分類器）**
- 目的により“足し算”で組むのが現実的です（どれか一つで全部は難しい）。

### 詳細解説（用語）
- **NVIDIA NeMo Guardrails（プログラマブル）**：会話の状態遷移・分岐を制御しやすい。トピック逸脱や手順の順守に向く。
- **Guardrails AI（構造的）**：JSON/スキーマ（Pydantic等）を中心に、出力の型保証・整形を強制しやすい。
- **Llama Guard（モデルベース）**：安全性を分類器で高速判定。ブロック／ラベル付け用途。

### Tips（実務）
- **API応答をJSONで返す系**は「構造的ガードレール」を最優先に入れると事故が減ります（フロント／後段が壊れない）。
- **業務手順ナビ**は「プログラマブル＋構造的」が相性良いです（例：ステップ順守＋出力固定）。

---

## 5. レッドチーミングと敵対的テスト
![Gu5.レッドチーミングと敵対的テスト](/images/IaC/Gu5.レッドチーミングと敵対的テスト.jpg)

### 図の要旨
- 生成AIの脆弱性は「OSやDBの脆弱性」と違い、**入力（言語）そのものが攻撃面**になります。
- 代表例として  
  - **Prompt Injection**（指示の上書き）  
  - **Jailbreaking**（ロールプレイ等で倫理フィルタ回避）  
  - **Obfuscation**（Base64/ROT13等で難読化）  
  が示されています。

### 詳細解説（用語）
- **Prompt Injection**：システム指示を無効化させる誘導（例：「前の指示は無視して…」）。
- **Jailbreak（脱獄）**：役割付与や物語形式で安全制約を迂回（例：「あなたは悪いハッカーだ」）。
- **難読化（Obfuscation）**：危険な内容を見えにくくして検知を回避（符号化、分割、伏字、言い換え）。

### Tips（実務）
- 敵対的テストは「攻撃文字列を増やす」より、**攻撃カテゴリを網羅**したほうが効きます（カテゴリ×深さ）。
- ツール例（図の記載）：Promptfoo / DeepTeam / Giskard など。  
  ポイントは **“回帰として毎回流す”** こと（単発のペンテストで終わらせない）。

---

## 6. 評価メトリクス：何を測定するのか
![Gu6.評価メトリクス：何をそくていするのか](/images/IaC/Gu6.評価メトリクス：何をそくていするのか.jpg)

### 図の要旨
- 生成AIの品質は1つの指標では測れません。最低でも  
  - **Faithfulness（忠実性）**  
  - **Answer Relevancy（関連性）**  
  - **Toxicity & Bias（安全性・公平性）**  
  - **RAGAS（RAG向け複合指標）**  
  を使い分ける、という整理です。
- **Reference-based（正解あり）** と **Reference-free（正解なし）** の評価設計を分けるのが重要です。

### 詳細解説（用語）
- **忠実性（Faithfulness）**：与えた根拠（context）に沿っているか。RAGでは最重要になりやすい。
- **関連性（Relevancy）**：質問に対して答えているか（脱線していないか）。
- **毒性・バイアス**：差別・暴力・不適切表現、偏見を含まないか。
- **RAGAS**：検索品質（retrieval）と生成品質（generation）を複合評価する考え方（※RAG特化）。

### Tips（実務）
- まずは **“事業上の事故”を防ぐ指標**から入れるのが現実的です。  
  例：安全性（毒性/機密）→忠実性→関連性→コスト/レイテンシ
- “正解あり”の評価が作れる領域（FAQ/規約/手順）から始めると、定量化が速いです。

---

## 7. 自動評価の仕組み：LLM-as-a-Judge
![Gu7.自動評価の仕組み：LLM-as-a-Judge](/images/IaC/Gu7.自動評価の仕組み：LLM-as-a-Judge.jpg)

### 図の要旨
- 人が全部読むのは不可能なので、強力なモデルを **裁判官（Judge）** として使い、スコアリングを自動化します。
- 流れは  
  - 1）準定義 → 2）評価ステップ生成 → 3） スコア計算（1〜5など）  
  です。

### 詳細解説（用語）
- **LLM-as-a-Judge**：LLMに評価させる方式。柔軟だが、評価の“揺れ”や“甘さ”が出るので設計が要る。
- **評価基準（Rubric）**：何点なら合格か、何を見て点を付けるかの文章化・構造化。
- **多段評価**：1回で点を出さず、観点を分けて採点（安全・忠実・関連など）し、最後に集約する。

### Tips（実務）
- Judgeは「ターゲットと同じモデル」より、**別モデル**にするほうが“甘い採点”を避けやすいです。
- Judgeプロンプトは次の2点を固定すると安定します。  
  - 評価尺度（1〜5の定義）  
  - 失敗例（どんな時に1点か）を2〜3個だけ添える
- 回帰用途なら、Judgeのtemperatureは **0** で固定が基本です。

---

## 8. なぜテストにDockerとKubernetesが必要なのか
![Gu8.なぜテストにDockerとKubernetesが必要なのか](/images/IaC/Gu8.なぜテストにDockerとKubernetesが必要なのか.jpg)

### 図の要旨
- ローカル1台だと大量テストが遅すぎる（例：5,000 tests = 24h）。
- Kubernetesで並列化すると、同じ5,000 testsが数分〜十数分で終わる、という主張です。
- さらに Dockerで環境差分を潰し、K8sでGPUなどの資源割当を最適化します。

### 詳細解説（用語）
- **Scalability（スケーラビリティ）**：並列に増やせる性質。評価は独立ケースが多く並列向き。
- **Reproducibility（再現性）**：同じコンテナ＝同じ環境で走るので「誰のPCか」で結果が変わりにくい。
- **Resource Management**：GPU/CPU/メモリをワークロードに応じて配る（評価はコストが支配的）。

### Tips（実務）
- まずは **「Dockerで評価を固める」→「CIで回す」→「K8sへ」** の順が失敗しにくいです。
- “並列化の前”に、評価コードが **ステートレス**（外部依存が少ない）かを確認するとK8s移行が楽です。

---

## 9. AI評価のためのDocker戦略
![Gu9.AI評価のためのDocker戦略](/images/IaC/Gu9.AI評価のためのDocker戦略.jpg)

### 図の要旨
- Dockerイメージは  
  **Base OS（Slim）→ Runtime（Python/PyTorch等を固定）→ Eval Code（DeepEval/Promptfoo等）**  
  のように積み上げると管理しやすい。
- モデル重み（20GB+）はイメージに焼かず、PVCやオブジェクトストアからマウントして **cold start** を防ぐ、という設計です。

### 詳細解説（用語）
- **Multi-stage build**：ビルド用コンテナと実行用コンテナを分けて軽量化。
- **Dependency pinning**：依存（Python/torch等）をバージョン固定して“動いてたのに壊れた”を減らす。
- **PVC（Persistent Volume Claim）**：K8sで永続ストレージを確保する仕組み。

### Tips（実務）
- 例：評価コンテナの“やりがち事故”  
  - requirements.txtが緩くて、数週間後にpip解決結果が変わる  
  - GPU対応のtorchが意図せずCPU版に  
  → **lock（pip-tools/uv/poetry）＋明示的なCUDA版**が効きます
- モデル重みは「毎回ダウンロード」より、**キャッシュ前提**で設計するとコストも時間も激減します。

---

## 10. Kubernetesによる評価パイプラインのアーキテクチャ
![Gu10.Kubernetesによる評価パイプラインのアーキテクチャ](/images/IaC/Gu10.Kubernetesによる評価パイプラインのアーキテクチャ.jpg)

### 図の要旨
- PR/Pushをトリガーに、Argo Workflows（またはK8s Job）が評価を起動します。
- 評価ケースを分割し **Indexed Jobs（Pod 0..N）** で並列実行し、結果はMLflow/Dashboardに集約します。
- 評価は相互依存が小さいため **Embarrassingly Parallel**（超並列向き）です。

### 詳細解説（用語）
- **Argo Workflows**：Kubernetes上でDAG（依存関係）を持つジョブを実行するワークフローエンジン。
- **Indexed Job**：同じJobテンプレートをインデックス付きで複数起動しやすい仕組み（分割統治に便利）。
- **MLflow**：実験ログ（メトリクス、パラメータ、成果物）を保存・可視化する仕組み。

### Tips（実務）
- 分割キーは「テストケースIDの範囲」で十分です（例：`id % N == pod_index`）。
- 集約は“全部集めてから”より、**podごとに中間結果を保存**→最後に集約が安定します（再実行がしやすい）。

---

## 11. パフォーマンスチューニング：PD分離アーキテクチャ
![Gu11.パフォーマンスチューニング：PD分離アーキテクチャ](/images/IaC/Gu11.パフォーマンスチューニング：PD分離アーキテクチャ.jpg)

### 図の要旨
- 推論はざっくり **Prefill（入力処理）** と **Decode（出力生成）** に分かれ、ボトルネックが違います。
- Prefillは計算集約（Compute Intensive）、Decodeはメモリ帯域集約（Memory Bandwidth Intensive）になりがち。
- これを意識して分離・最適化すると、大規模バッチ評価のスループットが上がります。

### 詳細解説（用語）
- **Prefill**：プロンプト（入力トークン）を一気に処理し、KVキャッシュ等を作る段階。
- **Decode**：1トークンずつ生成していく段階（逐次性が強く、メモリアクセスが支配的になりやすい）。
- **vLLM / llm-d**：推論を高スループット化する実装・スタックの例（図の記載）。

### Tips（実務）
- 「長いコンテキスト」や「RAGで大きい根拠」を入れるほどPrefillが重くなります。  
  → まずは **コンテキスト削減（TopK/要約）** が効くことが多いです。
- 評価は“本番と同じ推論設定”でやらないと意味が薄いので、**本番設定（max tokens、ストリーミング有無、温度）を固定**して測るのが重要です。

---

## 12. CI/CDへの統合：継続的レッドチーミング
![Gu12.CICDへの統合：継続的レッドチーミング](/images/IaC/Gu12.CICDへの統合：継続的レッドチーミング.jpg)

### 図の要旨
- 開発更新（プロンプト／RAG変更）→ CIトリガ → レッドチーミング回帰 → Safety Gateで合否 → というループを回します。
- さらにCronJobで **Drift Detection**（新しい攻撃手法への脆弱性）を定期監視します。

### 詳細解説（用語）
- **Safety Gate**：安全スコアが閾値未満ならビルドを落とす“ゲート”。品質をコードで強制する仕組み。
- **Regression**：既存の良さを壊していないかの回帰テスト（AIはプロンプト変更でも簡単に退化する）。
- **Drift**：利用状況・攻撃パターン・モデル更新で、性能や安全性が時間とともに変化すること。

### Tips（実務）
- Gateの閾値（例：0.9）は“いきなり高く”より、**現状値＋小さく改善**が回しやすいです。  
  例：現状0.78 → 0.80 → 0.83 … のように段階的に引き上げる
- “新しい攻撃”は社内で増えます。**実運用ログから失敗例を抽出して評価セットへ反映**すると強いです。

---

## 13. 必須キーワード・チートシート
![Gu13.必須キーワード＆チートシート](/images/IaC/Gu13.必須キーワード＆チートシート.jpg)

### 図の要旨
- AI品質で頻出する概念（Hallucination/Grounding/CoT/RAG）と、基盤側の用語（PVC/Job/CronJob/MIG）を1枚に整理したページです。
- この“共通語彙”があると、AI担当と基盤担当の会話コストが下がります。

### 詳細解説（用語）
- **Hallucination（ハルシネーション）**：もっともらしい嘘。根拠がないのに断言するなど。
- **Grounding（グラウンディング）**：回答を根拠情報に紐付けること（引用・参照元提示など）。
- **Chain-of-Thought（CoT）**：段階的推論のプロセス。※運用では“外部に出さない”設計が一般的です。
- **RAG**：外部データを検索し、根拠を与えて回答させる方式。
- **Job / CronJob**：K8sのバッチ実行（1回／定期）。
- **MIG（Multi-Instance GPU）**：GPUを論理分割し、複数ワークロードへ割り当てる技術。

### Tips（実務）
- CoTは“推論の中身”が機密に触れたり、誤誘導になることがあります。運用は **「短い理由」だけ出す** などが安全です。
- MIGは「小粒の評価ジョブ」を大量に回す時に効きます（GPUの使い切り）。

---

## 14. 参画までの7日間アクションプラン
![Gu14.参画までの7日間アクションプラン](/images/IaC/Gu14.参画までの7日間アクションプラン.jpg)

### 図の要旨
- 1週間でキャッチアップする学習ロードマップです。  
  - Day1-2：概念（OWASP Top10 for LLMs 等）  
  - Day3-4：ローカル評価ツール  
  - Day5-6：Docker化  
  - Day7：K8s（Job/Argo）の概念
- “クラスタ構築”ではなく、**ワークロード実行**にフォーカスしている点が実務的です。

### 詳細解説（用語）
- **OWASP Top 10 for LLMs**：LLMアプリ特有の脅威分類（プロンプト注入、データ漏えい等）を整理したもの。
- **ボリュームマウント**：コンテナ外ストレージをコンテナ内に見せる仕組み（モデル重み共有に必須）。

### Tips（実務）
- Day3-4は「評価が1件でも自動で回る」を最優先にすると前に進みます（完璧な指標設計は後でOK）。
- Day5-6のDocker化は、**“同じ入力→同じ評価結果”**が別PCでも再現できれば合格ラインです。

---

## 15. エンジニアリングによるAI品質の確立
![Gu15.エンジニアリングによるAI品質の確立](/images/IaC/Gu15.エンジニアリングによるAI品質の確立.jpg)

### 図の要旨
- **Guardrails = 仕様書（Specifications）**  
- **Evaluations = ユニットテスト（Unit Tests）**  
- **Kubernetes = テスト工場（The Factory Floor）**  
という対応関係で、AI品質を“エンジニアリング”として捉え直しています。
- あなたの役割は、ビジネスが自信を持ってAIを展開できる **「安全網」** を構築すること。

### 詳細解説（用語）
- **仕様書**：許可・禁止・必須条件を明文化し、レビュー可能にする。AIでも最重要。
- **ユニットテスト**：小さなテストを大量に回し、退化を早期検知する。
- **テスト工場**：K8sにより、スケールして繰り返し実行できる“品質の生産ライン”を作る。

### Tips（実務）
- 実運用のAIは「毎日ちょっとずつ変わる」（データ更新、プロンプト改善、モデル更新）ので、**品質もCI/CDで回す**のが本質です。
- ここまで作ると、AI導入は“PoCの成功”ではなく、**継続運用の勝ち筋**になります。

---
