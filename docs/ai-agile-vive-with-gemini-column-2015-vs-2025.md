# 【コラム】2015年のAIと、今のAIは何が違うのか？

## —— 「道具」から「相棒」へ進化した技術的理由

先日、福岡支店での講演（Vive-with-Gemini）の際、2015年頃からAI開発に携わっていた方から、鋭い質問をいただきました。

> **「今のAIも、結局は過去のデータから相関関係を見ているだけですよね？ 因果関係を理解しているわけではないのに、なぜこれほど『対話』が成立するのですか？」**

この質問は、AIの本質を突いています。
2015年当時のAI（ディープラーニング初期）を知る技術者にとって、AIとは「データを分類する『道具』」でした。しかし、今の私たちが実践している Vive-with-Gemini は、AIを「共に考える『相棒』」として扱います。

なぜ、AIは「道具」から「相棒」へと格上げされたのか？
その背景には、<strong>「計算の物理的な質的変化」と「使い方の革命」</strong>がありました。

-----

## 1\. 2015年の常識：「相関関係の計算機」

2015年頃、AI（CNNやRNN）は「猫の画像」を見て「猫」と答えることはできましたが、「なぜ猫なのか」を説明することはできませんでした。
当時の認識通り、AIの仕組みは<strong>「統計的な相関関係（パターンマッチング）」</strong>であり、それは現在も変わっていません。

AIは言葉の意味を理解しているわけではありません。すべての言葉を<strong>「ベクトル（数百〜数千次元の数字の列）」**に変換し、ベクトル同士の**「向き（コサイン類似度）」</strong>が近いかどうかで、「意味の近さ」を判定しています。
この「ベクトル計算」こそがAIの実体です。

## 2\. 2017年の革命と「NVIDIA一強」の理由

転機は2017年、<strong>Transformer（トランスフォーマー）</strong>の登場でした。これがすべてを変えました。

それまでのAI（RNN）は、文章を前から順に計算する「伝言ゲーム」でした。
しかしTransformerは、文章中の<strong>すべての単語ベクトル同士の掛け算（行列演算）を、「せーの！」で一気に（並列に）</strong>行う仕組みでした。

### 核心技術：Attentionと文脈学習

このTransformerの中心にあるのが、**Attention（注意機構）** です。
これは、文章の中の「どの単語」と「どの単語」が強く関連しているかを、あたかも人間が注目するように計算する仕組みです。
この機構のおかげで、AIは再学習をすることなく、プロンプトに入力された情報をその場で理解してタスクに適応する **文脈学習（In-Context Learning）** という能力を手に入れました。私たちがAIに「この規約を守って」と指示するだけで従うのは、この能力のおかげです。

### 盤石な基盤：CUDA

ここでハードウェアの運命が決まりました。この巨大な並列計算を処理できるのは、CPU（指揮官）ではなく、単純な計算を大量にこなすGPU（数千人の兵隊）だけだったからです。

結果として、GPU市場を独占していた**NVIDIA**が「AIの頭脳」の供給元となりました。
さらに重要なのは、彼らがハードウェアだけでなく、**CUDA** という並列計算のためのソフトウェア基盤を長年提供し続けていたことです。世界中のAI研究者がCUDA上で開発を行っていたため、このエコシステムが「NVIDIA一強」の状態を盤石なものにしました。

## 3\. 量が質を変えた：「創発」という現象

さらに、<strong>Scaling Laws（スケーリング則）</strong>という発見がありました。
「GPUを並べてデータと計算量を増やせば増やすほど、AIは賢くなる」という法則です。

この法則の発見は、AI開発の競争ルールを一変させました。「計算リソースを投下すれば、その分だけ確実に性能が向上する」という見通しが立ったため、現在のような世界中の企業が競ってGPUへの巨額投資を行う<strong>「物量競争」</strong>が加速することになったのです。

そして、その物量の果てに、インターネット上のほぼ全てのテキストを学習させた結果、<strong>「創発（Emergence）」</strong>という現象が起きました。
仕組み上は「次に来る言葉を確率で予測しているだけ」なのに、あたかも**「因果関係を理解し、論理的に推論しているかのような振る舞い」\*\*を獲得したのです。

  * **Before:** 特定タスク専用のモデルを、職人芸で「作る」時代。
  * **After:** すでに博識な巨大モデルに、指示を出して「指揮する」時代。

## 4\. 現場での活用技術：RAGとLoRA

この「巨大な汎用脳」を、私たちの現場（Vive-with-Gemini）で「相棒」にするために使われているのが、**RAG**と**LoRA**です。

  * **RAG（検索拡張生成）：**
    AIは賢いですが、私たちのプロジェクトのことは知りません。そこで、社内規約や設計書を検索させ、それを「カンニングペーパー」として渡すことで、文脈学習（In-Context Learning）を最大限に活かした<strong>「文脈を知っている相棒」</strong>にします。
  * **LoRA（ローラ）：**
    巨大なモデル本体（脳）を再学習させるのは数億円かかります。そこで、脳の外側に「薄いフィルター（アダプター）」だけをくっつけて、そこだけを学習させる技術です。これにより、低コストで\*\*「チーム専用の性格」\*\*を持たせることができます。

## 5\. そして未来へ：「学習」から「推論」の時代

最後に、これからのトレンドについて触れます。
これまでは、賢いAIを作るための<strong>「学習（Training）」</strong>に大量のGPUが必要とされ、そこにお金が集まっていました。

しかしこれからは、社会に出たAIが毎日仕事をする<strong>「推論（Inference）」</strong>のフェーズに入ります。
私たちがコードを書かせたりレビューをさせたりするたびに、裏側ではGPUが推論の計算を回しています。ここには2つの潮流があります。

1.  **クラウド推論:** Geminiのような超高性能なモデルを、データセンターの巨大なGPU群で動かす。
2.  **エッジ推論:** AI PCやスマートフォンなど、ユーザーの手元の端末（エッジ）で、通信ラグなしにAIを動かす。

さらに今後は、これらが独立して動くだけでなく、ユーザーの指示の難易度やセキュリティ要件を瞬時に判断し、「軽い処理はエッジで即答」「複雑な思考はクラウドへ依頼」といった具合に、最適な実行場所へリクエストを振り分ける<strong>「オーケストレーション（AIルーティング）」</strong>の使い方が重要になります。

Vive-with-Gemini のような対話型開発が一般的になれば、クラウドとエッジをハイブリッドに組み合わせることで、**学習よりも推論のために必要なGPUリソースの方が圧倒的に多くなる**と言われています。
「作る競争」から「使い倒す競争」へ。フェーズは確実に変わっています。

-----

## 結論：Vive-with-Gemini が「相棒」と呼ぶ理由

2015年のAIは、特定の作業を自動化する優れた「道具」でした。
2025年のAIは、**GPUとCUDA**による計算力を背景に、**Attention機構**で文脈を深く理解し、**RAG**で現場の知識を武装した、頼れる<strong>「相棒」</strong>です。

かつての技術を知る方こそ、この「中身は相関関係の行列計算なのに、振る舞いが人間のような相棒になった」という進化の面白さを、ぜひ現場のペアプロを通じて体感していただければと思います。

-----

👉 [前の記事：「現場で磨かれるAI活用術：Vive-with-Gemini実践編」に戻る](https://www.google.com/search?q=./ai-agile-vive-with-gemini-extended)